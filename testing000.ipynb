{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fec6c0e",
   "metadata": {},
   "source": [
    "My first neural network:\n",
    "\n",
    "* Firstly splitting and preprocessing the data\n",
    "\n",
    "* Creating the neural network from scratch\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Gained ~95% accuracy with training loop\n",
    "* ~75% accuracy wiht testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e015bc210445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T17:45:46.046116Z",
     "start_time": "2025-01-31T17:45:46.037491Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# function to retrieve data, specificallly for simple classification\n",
    "def retrieve_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    dataset = data.values\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = retrieve_data('GermanCredit.csv')\n",
    "\n",
    "#columns that are categorical, numerical, and binary (minus the output column)\n",
    "categoricals = np.array([0,2,3,5,6,8,9,11,13,14,16])\n",
    "numericals =np.array([1,4,12,15,17])\n",
    "binaries = np.array([18,19])\n",
    "\n",
    "\n",
    "#splitting test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# data pipelining and transformers:\n",
    "# i.e. OneHotEncoding categorical and binary vars and standardizing numerical vars \n",
    "\n",
    "# one hot encoder\n",
    "pre = ColumnTransformer(\n",
    "    transformers =[\n",
    "        ('cat', OneHotEncoder(sparse_output=True), categoricals),\n",
    "        ('num', StandardScaler(), numericals),\n",
    "        ('bin', OneHotEncoder(), binaries)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', pre)\n",
    "])\n",
    "\n",
    "#using pipeline on both training and testing datasets\n",
    "X_train_trans = pipeline.fit_transform(X_train)\n",
    "X_test_trans = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae5295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (800, 20)\n",
      "Transformed X_train shape: (800, 59)\n",
      "Number of features in transformed X_train: 59\n",
      "Categorical columns unique values:\n",
      "Column 0 unique values: 4\n",
      "Column 2 unique values: 5\n",
      "Column 3 unique values: 10\n",
      "Column 5 unique values: 5\n",
      "Column 6 unique values: 5\n",
      "Column 8 unique values: 4\n",
      "Column 9 unique values: 3\n",
      "Column 11 unique values: 4\n",
      "Column 13 unique values: 3\n",
      "Column 14 unique values: 3\n",
      "Column 16 unique values: 4\n",
      "(800, 1)\n",
      "Batch size: (1000, 20)\n",
      "Batch size: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# testing shapes and amt of unique values in the categorical columns\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Transformed X_train shape:\", X_train_trans.shape)\n",
    "print(f\"Number of features in transformed X_train: {X_train_trans.shape[1]}\")\n",
    "print(\"Categorical columns unique values:\")\n",
    "for col in categoricals:\n",
    "    print(f\"Column {col} unique values: {len(np.unique(X[:, col]))}\")\n",
    "print(y_train.shape)\n",
    "\n",
    "batch_size = X.shape\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "batch_sizey = y.shape\n",
    "print(\"Batch size:\", batch_sizey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ca3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starter neural network\n",
    "\n",
    "#initializing biases and weights based off the dimensions of the input and output data\n",
    "class neural():\n",
    "    def __init__(self, inputs_dim, outputs_dim, hidden_dim1, hidden_dim2):\n",
    "        \n",
    "        # setting hidden weights for hidden layer\n",
    "        self.hidden_weights1 = self.glorot_weights(inputs_dim, hidden_dim1)\n",
    "        self.hidden_bias1= np.zeros((1, hidden_dim1), dtype=np.float64)\n",
    "\n",
    "        self.hidden_weights2 = self.glorot_weights(hidden_dim1, hidden_dim2)\n",
    "        self.hidden_bias2 = np.zeros((1, hidden_dim2), dtype=np.float64)\n",
    "\n",
    "        # output layer weights and bias\n",
    "        self.output_weights = self.glorot_weights(hidden_dim2, outputs_dim)\n",
    "        self.output_bias = np.zeros((1, outputs_dim), dtype=np.float64)\n",
    "        \n",
    "    # weight initialization from the xavier glorot paper\n",
    "    def glorot_weights(self, input_dim, output_dim):\n",
    "        limit = np.sqrt(6 / (input_dim + output_dim))\n",
    "        \n",
    "        # using random uniform values to prevent gradients from exploding and maintaining stability\n",
    "        return np.random.uniform(-limit, limit, (output_dim, input_dim)).astype(np.float64)\n",
    "\n",
    "    # activation function to introduce nonlinearity\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #help computing gradient, gives directions to weight and bias update based on value given (x)\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    # feed forward function\n",
    "    def forward(self, inputs, training=True):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        #layer 1 \n",
    "        self.layer_1 = self.relu(np.dot(inputs, self.hidden_weights1.T) + self.hidden_bias1)\n",
    "        \n",
    "        if training:\n",
    "            self.layer_1 = self.dropout(self.layer_1)\n",
    "        \n",
    "        # layer 2\n",
    "        self.layer_2 = self.relu(np.dot(self.layer_1, self.hidden_weights2.T) + self.hidden_bias2)\n",
    "\n",
    "        # layer 3 (output layer)\n",
    "        self.layer_3 = self.sigmoid(np.dot(self.layer_2, self.output_weights.T) + self.output_bias)\n",
    "        return self.layer_3\n",
    "    \n",
    "    # loss function\n",
    "    def bce_loss(self, y, y_hat):\n",
    "        epsilon = 1e-8  # to avoid log(0)\n",
    "        return -np.mean(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "    def dropout(self, layer, rate= 0.25):\n",
    "        mask = (np.random.rand(*layer.shape) > rate).astype(np.float64)\n",
    "        return layer * mask / (1 - rate)\n",
    "    \n",
    "    # backpropagation, computing gradients and error, updates both + weights/biases\n",
    "    def backward(self, y, y_hat, learning_rate=0.0001):\n",
    "        \n",
    "        #error = y_hat - y\n",
    "        #output_gradient = error * (y_hat * (1 - y_hat))\n",
    "        \n",
    "        output_gradient = y_hat - y\n",
    "        self.output_weights -= learning_rate * np.dot(output_gradient.T, self.layer_2)\n",
    "        self.output_bias -= learning_rate * np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        # gradient for hidden layer 2\n",
    "        hidden_error2 = np.dot(output_gradient, self.output_weights)  # Fixed here\n",
    "        hidden_gradient2 = hidden_error2 * self.relu_derivative(self.layer_2)\n",
    "        \n",
    "        # update hidden layer 2 weights and bias\n",
    "        self.hidden_weights2 -= learning_rate * np.dot(hidden_gradient2.T, self.layer_1)\n",
    "        self.hidden_bias2 -= learning_rate * np.sum(hidden_gradient2, axis=0, keepdims=True)\n",
    "\n",
    "        # compute gradient for hidden layer 1\n",
    "        hidden_error1 = np.dot(hidden_gradient2, self.hidden_weights2.T)\n",
    "        hidden_gradient1 = hidden_error1 * self.relu_derivative(self.layer_1)\n",
    "        \n",
    "        # update hidden layer 1 weights and bias\n",
    "        self.hidden_weights1 -= learning_rate * np.dot(hidden_gradient1.T, self.inputs)\n",
    "        self.hidden_bias1 -= learning_rate * np.sum(hidden_gradient1, axis=0, keepdims=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7b2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(X, y):\n",
    "    \n",
    "    X = np.array(X, dtype=np.float64)\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "    \n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    hidden_dim1 = 16\n",
    "    hidden_dim2 = 16\n",
    "    output_dim = y.shape[1]\n",
    "\n",
    "    # def's instance of neural\n",
    "    n = neural(input_dim, output_dim, hidden_dim1, hidden_dim2)\n",
    "    epochs = 50001\n",
    "    \n",
    "    # training using batches\n",
    "    batch_size = 64\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for start in range(0, X.shape[0], batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            predictions = n.forward(X_batch)\n",
    "            loss = n.bce_loss(y_batch, predictions)\n",
    "            n.backward(y_batch, predictions)\n",
    "        if epoch % 10000 == 0:\n",
    "            print(f\"epoch {epoch}/{epochs -1}, loss: {loss}\")\n",
    "            \n",
    "\n",
    "    # training normally\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        predictions = n.forward(X)\n",
    "        loss = n.bce_loss(y, predictions)\n",
    "        n.backward(y, predictions)\n",
    "    \n",
    "        if epoch % 10000 == 0:\n",
    "            print(f\"epoch {epoch}/{epochs -1}, loss: {loss}\")\n",
    "    '''\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bb83e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/50000, loss: 0.6790791504799314\n",
      "epoch 10000/50000, loss: 0.25826133440179255\n",
      "epoch 20000/50000, loss: 0.11747269470526343\n",
      "epoch 30000/50000, loss: 0.040812720526524356\n",
      "epoch 40000/50000, loss: 0.1279028170818199\n",
      "epoch 50000/50000, loss: 0.08054158309288809\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model = train(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca37ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating it\n",
    "def evaluate(model, X, y):\n",
    "\n",
    "    y = np.array(y, dtype=np.float64)\n",
    "\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "    predictions = model.forward(X, training=False)\n",
    "    loss = model.bce_loss(y, predictions)\n",
    "\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(predicted_labels == y)\n",
    "    \n",
    "    print(f\"Evaluation Loss: {loss}\")\n",
    "    print(f\"Evaluation Accuracy: {accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a284bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 2.3096013530009762\n",
      "Evaluation Accuracy: 72.0%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, X_test_trans, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
